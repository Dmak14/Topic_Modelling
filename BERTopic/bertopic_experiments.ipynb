{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bertopic Experiment Notebook - Reduced Dataset for quick processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd \n",
    "\n",
    "# Dataset Path \n",
    "# Data Downloaded from https://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data\n",
    "data_path = r\"C:\\TEMP\\REPO\\Topic_Modelling\\Topic_Modelling\\BERTopic\\datasets\\complaints.csv\"\n",
    "\n",
    "# Filter Columns\n",
    "cols_to_keep = ['Date received','Product','Sub-product','Issue','Sub-issue','Consumer complaint narrative', 'Company public response', 'Company response to consumer', 'Tags', 'Company']\n",
    "\n",
    "# Filter dataset condition where length of complaint narrative <=5000 (As per Max form character input)\n",
    "# Ingest data by chunks and filter\n",
    "chunks = pd.read_csv(data_path, \n",
    "                 usecols=cols_to_keep,\n",
    "                 chunksize=5000)\n",
    "df = pd.concat(chunk[chunk['Consumer complaint narrative'].str.len() < 5000] for chunk in chunks)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by one company's complaints only\n",
    "df_date = df[df['Company'] == 'WELLS FARGO & COMPANY']\n",
    "\n",
    "# Drop rows where customer complaints were null\n",
    "df_date = df_date.dropna(subset='Consumer complaint narrative')\n",
    "\n",
    "# Create year month column to filter by\n",
    "df_date['Date received'] = pd.to_datetime(df_date['Date received'])\n",
    "df_date['year_month'] = df_date['Date received'].dt.to_period('M')\n",
    "\n",
    "# Display row counts per month\n",
    "grouped_df = df_date.groupby('year_month').size().reset_index(name='counts')\n",
    "filtered_df = grouped_df[grouped_df['year_month'] > '2023']\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter latest complaint data to filter volumes to something more realistic\n",
    "rdf = df_date[(df_date['year_month'] > '2024-06')]\n",
    "rdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for DF viewing (wrap text)\n",
    "pd.set_option('display.max_colwidth', None)  # Set to None to display full text\n",
    "pd.set_option('display.width', 1000)  # Set the display width to a large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA and an initial model output showed patterns of PI that converted to X's that was coming through to the output due to the prevalence \n",
    "    # Therefore, Remove any pattern of 'X's in 'pii_column'\n",
    "    # And Replace XX/XX/XXXX as [DATE]\n",
    "\n",
    "rdf['Consumer complaint narrative preprocessed'] = rdf['Consumer complaint narrative'].str.replace(r'XX/XX/\\d{4}', '[DATE]', regex=True)\n",
    "rdf['Consumer complaint narrative preprocessed'] = rdf['Consumer complaint narrative preprocessed'].str.replace('X+', '', regex=True)\n",
    "rdf['Consumer complaint narrative preprocessed'] = rdf['Consumer complaint narrative preprocessed'].str.replace('//', ' ', regex=True)\n",
    "rdf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online resources for improving the performance of BERTopic \n",
    "1. https://github.com/MaartenGr/BERTopic/issues/423 - Reducing the number of outliers\n",
    "2. https://maartengr.github.io/BERTopic/faq.html - FAQ's (including reducing the number of outliers)\n",
    "3. https://github.com/UKPLab/sentence-transformers/issues/888 - Using SentenceTransformers offline\n",
    "4. https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#finding-similar-topics-between-models - Tips and tricks (Further approaches for refining the outputs and improving performance)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "### Train BERTopic - Initial Test\n",
    "Data volumes ~ 1300 documents/complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = rdf['Consumer complaint narrative preprocessed'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "# from keybert import KeyBERT\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit the model on the text data\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Get topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Display the topics\n",
    "print(topic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 Results\n",
    "1. Only 1 topic was generated from a count of 29 complaints\n",
    "2. Volumes may be too low to get any meaninngful embeddings\n",
    "3. Topic includes pretty much all stop words and no meaningful insights \n",
    "4. The visual shows sparsity in the output topics meaning either the data requires preprocessing or there is an issue with the clustering algorithm on this data.\n",
    "\n",
    "Poor preprocessing can lead to poor topic modelling results with BERTopic. Preprocessing techniques like removing stopwords, stemming and lemmatization could be tried.\n",
    "Preprocessing can help reduce noise and improve embedding quality ensuring the embeddings capture the essential semantics of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the formatting of the Topic Names\n",
    "topic_labels = topic_model.generate_topic_labels(nr_words=4, separator=\" - \")\n",
    "topic_model.set_topic_labels(topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Topics\n",
    "topic_model.visualize_barchart(width=350, height=430, top_n_topics=8, n_words=10, custom_labels=topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents and Topics\n",
    "topic_model.visualize_documents(docs, topics=list(range(17)), custom_labels=True, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.1 - Reducing min_topic_size to reduce the number of documents placed into each cluster \n",
    "1. This has significantly improved the intepretability of the model outputs, the topics and the topic representations\n",
    "2. min_topic_size should be reduced and makes sense to be explicity specified for smaller volumes of documents (~1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "# from keybert import KeyBERT\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic(min_topic_size=10)\n",
    "\n",
    "# Fit the model on the text data\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Get topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Display the topics\n",
    "print(topic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "Removing Stopwords due to poor topic interpretability in Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "srdf = rdf\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "srdf['Consumer complaint narrative preprocessed'] = srdf['Consumer complaint narrative preprocessed'].apply(remove_stopwords)\n",
    "\n",
    "srdf.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = srdf['Consumer complaint narrative preprocessed'].to_list()\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model_2 = BERTopic()\n",
    "\n",
    "# Fit the model on the text data\n",
    "topics, probs = topic_model_2.fit_transform(docs)\n",
    "\n",
    "# Get topic information\n",
    "topic_info = topic_model_2.get_topic_info()\n",
    "\n",
    "# Change the formatting of the Topic Names\n",
    "topic_labels = topic_model_2.generate_topic_labels(nr_words=4, separator=\" - \")\n",
    "topic_model_2.set_topic_labels(topic_labels)\n",
    "\n",
    "# Display the topics\n",
    "topic_model_2.visualize_barchart(width=400, height=430, top_n_topics=8, n_words=10, custom_labels=topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 Results\n",
    "1. Removed stopwords seemed to have improved performance significantly upon visual analysis of outputted topics\n",
    "2. Topics and topic representations (words) seem to be distinct, interpretable themes \n",
    "3. 16, non-outlier topics\n",
    "4. Visualisation below shows distict clusters of documents relating to their representations\n",
    "5. A few representations include duplicate words with similar stems. E.g. Fraud - Fraudulent, Check - Checks  - This could potentially be solved by pre-processing using stemming and lematisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents and Topics\n",
    "topic_model_2.visualize_documents(docs, topics=list(range(17)), custom_labels=True, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3\n",
    "n-gram range = (1,3-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change n-grams to (1,3) without re-fitting entire model - The default n-gram range for BERTopic is (1, 2).\n",
    "topic_model_2.update_topics(docs, n_gram_range=(1,3))\n",
    "# Display the topics\n",
    "topic_model_2.visualize_barchart(width=400, height=430, top_n_topics=8, n_words=10, custom_labels=topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 outputs - visual analysis\n",
    "1. Using an n-gram range of (1,3) over the default of (1,2) allow for a few combinations of the words we have already seen in the representations but this gives a bit more interpretability in my opinion. E.g. For topic 1 - 'debit card' (as opposed to 'debit' by itself - meaning debit into an account) and topic 6 - 'closed account' & 'credit card' (as opposed to 'credit' by itself - meaning credit to an account or credit reporting)\n",
    "2. 'wells', 'fargo' and 'wells fargo' appear potentially too often in each of the topics so it's worth removing them as stopwords.\n",
    "3. There is very little difference between the topics and their representations of (1,3) and (1,4), but the topics, it's representations and the latent meaning behind each for (1,5) does seem to shift which is interesting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change n-grams to (1,4) without re-fitting entire model - The default n-gram range for BERTopic is (1, 2).\n",
    "topic_model_2.update_topics(docs, n_gram_range=(1,4))\n",
    "# Display the topics\n",
    "topic_model_2.visualize_barchart(width=400, height=430, top_n_topics=8, n_words=10, custom_labels=topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change n-grams to (1,5) without re-fitting entire model - The default n-gram range for BERTopic is (1, 2).\n",
    "topic_model_2.update_topics(docs, n_gram_range=(1,5))\n",
    "# Display the topics\n",
    "topic_model_2.visualize_barchart(width=400, height=430, top_n_topics=8, n_words=10, custom_labels=topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4\n",
    "Stemming and Lemmatisation for n-gram range (1,3) \n",
    "\n",
    "1. Lemmatization First: Converts words to their base forms considering the context.\n",
    "2. Stemming Second: Further reduces words to their root forms if needed.\n",
    "\n",
    "Note: Lemmatization should be applied first to ensure that words are converted to their base forms considering the context, and then stemming can be applied if further reduction is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Wells Fargo as a stopword\n",
    "import re\n",
    "\n",
    "def remove_custom_stopwords(text, stopwords):\n",
    "    # Create a regex pattern for the stopwords\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(word) for word in stopwords) + r')\\b', re.IGNORECASE)\n",
    "    # Remove the stopwords\n",
    "    return pattern.sub('', text).strip()\n",
    "\n",
    "# List of custom stopwords\n",
    "custom_stopwords = ['wells', 'fargo', 'wells fargo', 'well']\n",
    "\n",
    "docs = srdf['Consumer complaint narrative preprocessed'].to_list()\n",
    "\n",
    "cleaned_complaints = [remove_custom_stopwords(text, custom_stopwords) for text in docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization Only - Contextual reduction of words to their base/dictionary form\n",
    "As to reduce repeated similar words.\n",
    "\n",
    "1. Stopwords were already previously removed (Including Wells Fargo)\n",
    "2. Applied lemmatization only (Not Stemming)\n",
    "3. Applied min_topic_size=10 & n_gram_range=(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "vectorizer_model = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "topic_model_lemstem = BERTopic(vectorizer_model=vectorizer_model, min_topic_size=10, n_gram_range=(1,3))\n",
    "\n",
    "# Fit the model on the text data\n",
    "topics, probs = topic_model_lemstem.fit_transform(cleaned_complaints)\n",
    "\n",
    "# Get topic information\n",
    "topic_info_lemstem = topic_model_lemstem.get_topic_info()\n",
    "\n",
    "# Change the formatting of the Topic Names\n",
    "topic_labels_lemstem = topic_model_lemstem.generate_topic_labels(nr_words=4, separator=\" - \")\n",
    "topic_model_lemstem.set_topic_labels(topic_labels_lemstem)\n",
    "\n",
    "# Display the topics\n",
    "topic_model_lemstem.visualize_barchart(width=400, height=430, top_n_topics=8, n_words=10, custom_labels=topic_labels_lemstem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model_lemstem.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents and Topics\n",
    "topic_model_lemstem.visualize_documents(docs, topics=list(range(17)), custom_labels=True, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of Lemmatising only experiment\n",
    "1. Highest performance on interpretability seen out of all the experiments so far\n",
    "2. 0 topic can be further refined as the topic representations for this topic are vague and volumes are high - The visualisation of the cluster shows a disparsed cluster of complaints/documents which further reinforces this\n",
    "3. -1, outlier, topic can be further refined too.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Outputs and visualisations for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud for Topic 3\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_wordcloud(model, topic):\n",
    "    text = {word: value for word, value in model.get_topic(topic)}\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    wc.generate_from_frequencies(text)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Show wordcloud\n",
    "create_wordcloud(topic_model_lemstem, topic=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 10 complaints assigned to topic 3\n",
    "document_info_df = topic_model_lemstem.get_document_info(docs=cleaned_complaints)\n",
    "document_info_df = document_info_df[document_info_df['Topic'] ==3][['Document', 'Topic', 'Name', 'Probability']][0:9]\n",
    "\n",
    "pre_docs = rdf['Consumer complaint narrative'].to_list()\n",
    "\n",
    "# Link each document back to the original docs list\n",
    "document_info_df['Original Document'] = document_info_df.index.map(lambda idx: pre_docs[idx])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "document_info_df[['Document', 'Original Document', 'Topic', 'Name', 'Probability']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# Function to apply stemming and lemmatization\n",
    "def lemmatize_and_stem(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed = [stemmer.stem(token) for token in lemmatized]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "# Apply the function to the list of docs\n",
    "processed_docs = [lemmatize_and_stem(complaints) for complaints in cleaned_complaints]\n",
    "\n",
    "print(docs[0])\n",
    "print(processed_docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model_lemstem = BERTopic()\n",
    "\n",
    "# Fit the model on the text data\n",
    "topics, probs = topic_model_lemstem.fit_transform(processed_docs)\n",
    "\n",
    "# Get topic information\n",
    "topic_info_lemstem = topic_model_lemstem.get_topic_info()\n",
    "\n",
    "# Change the formatting of the Topic Names\n",
    "topic_labels_lemstem = topic_model_lemstem.generate_topic_labels(nr_words=4, separator=\" - \")\n",
    "topic_model_lemstem.set_topic_labels(topic_labels_lemstem)\n",
    "\n",
    "# Display the topics\n",
    "topic_model_lemstem.visualize_barchart(width=400, height=430, top_n_topics=8, n_words=10, custom_labels=topic_labels_lemstem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5\n",
    "KeyBERT with n-gram (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# Initialize BERTopic with custom settings\n",
    "representation_model = KeyBERTInspired()\n",
    "Key_topic_model = BERTopic(representation_model=representation_model, calculate_probabilities=True, n_gram_range=(1, 3))\n",
    "\n",
    "docs = srdf['Consumer complaint narrative preprocessed'].to_list()\n",
    "\n",
    "# Fit the model on your documents\n",
    "topics, probs = Key_topic_model.fit_transform(docs)\n",
    "\n",
    "# # Save the trained topic model for later so training is not required again\n",
    "# Key_topic_model.save(r\"C:\\TEMP\\REPO\\Topic_Modelling\\Topic_Modelling\\BERTopic\\models\\test_cf_complaints_KEYbertopic_model_small\")\n",
    "\n",
    "# Get topic information\n",
    "key_topic_info = Key_topic_model.get_topic_info()\n",
    "\n",
    "# Change the formatting of the Topic Names\n",
    "key_topic_labels = Key_topic_model.generate_topic_labels(nr_words=4, separator=\" - \")\n",
    "Key_topic_model.set_topic_labels(key_topic_labels)\n",
    "\n",
    "# Display the topics\n",
    "Key_topic_model.visualize_barchart(width=400, height=430, top_n_topics=8, n_words=10, custom_labels=key_topic_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key_topic_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
